{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "444644de-c5ac-4119-a73a-0af0bfc345e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f060413-e6ab-4429-9a23-427af5a91aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>...</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>...</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>...</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2008-10-16 15:13:00</td>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>100.0</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>...</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>2008-10-16 20:49:00</td>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2008-10-17 05:26:00</td>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2008-10-17 06:01:00</td>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>...</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2008-10-17 06:07:00</td>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>100.0</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows Ã— 592 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time        0        1          2          3       4  \\\n",
       "0     2008-07-19 11:55:00  3030.93  2564.00  2187.7333  1411.1265  1.3602   \n",
       "1     2008-07-19 12:32:00  3095.78  2465.14  2230.4222  1463.6606  0.8294   \n",
       "2     2008-07-19 13:17:00  2932.61  2559.94  2186.4111  1698.0172  1.5102   \n",
       "3     2008-07-19 14:43:00  2988.72  2479.90  2199.0333   909.7926  1.3204   \n",
       "4     2008-07-19 15:22:00  3032.24  2502.87  2233.3667  1326.5200  1.5334   \n",
       "...                   ...      ...      ...        ...        ...     ...   \n",
       "1562  2008-10-16 15:13:00  2899.41  2464.36  2179.7333  3085.3781  1.4843   \n",
       "1563  2008-10-16 20:49:00  3052.31  2522.55  2198.5667  1124.6595  0.8763   \n",
       "1564  2008-10-17 05:26:00  2978.81  2379.78  2206.3000  1110.4967  0.8236   \n",
       "1565  2008-10-17 06:01:00  2894.92  2532.01  2177.0333  1183.7287  1.5726   \n",
       "1566  2008-10-17 06:07:00  2944.92  2450.76  2195.4444  2914.1792  1.5978   \n",
       "\n",
       "          5         6       7       8  ...       581     582     583     584  \\\n",
       "0     100.0   97.6133  0.1242  1.5005  ...       NaN  0.5005  0.0118  0.0035   \n",
       "1     100.0  102.3433  0.1247  1.4966  ...  208.2045  0.5019  0.0223  0.0055   \n",
       "2     100.0   95.4878  0.1241  1.4436  ...   82.8602  0.4958  0.0157  0.0039   \n",
       "3     100.0  104.2367  0.1217  1.4882  ...   73.8432  0.4990  0.0103  0.0025   \n",
       "4     100.0  100.3967  0.1235  1.5031  ...       NaN  0.4800  0.4766  0.1045   \n",
       "...     ...       ...     ...     ...  ...       ...     ...     ...     ...   \n",
       "1562  100.0   82.2467  0.1248  1.3424  ...  203.1720  0.4988  0.0143  0.0039   \n",
       "1563  100.0   98.4689  0.1205  1.4333  ...       NaN  0.4975  0.0131  0.0036   \n",
       "1564  100.0   99.4122  0.1208     NaN  ...   43.5231  0.4987  0.0153  0.0041   \n",
       "1565  100.0   98.7978  0.1213  1.4622  ...   93.4941  0.5004  0.0178  0.0038   \n",
       "1566  100.0   85.1011  0.1235     NaN  ...  137.7844  0.4987  0.0181  0.0040   \n",
       "\n",
       "          585     586     587     588       589  Pass/Fail  \n",
       "0      2.3630     NaN     NaN     NaN       NaN         -1  \n",
       "1      4.4447  0.0096  0.0201  0.0060  208.2045         -1  \n",
       "2      3.1745  0.0584  0.0484  0.0148   82.8602          1  \n",
       "3      2.0544  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "4     99.3032  0.0202  0.0149  0.0044   73.8432         -1  \n",
       "...       ...     ...     ...     ...       ...        ...  \n",
       "1562   2.8669  0.0068  0.0138  0.0047  203.1720         -1  \n",
       "1563   2.6238  0.0068  0.0138  0.0047  203.1720         -1  \n",
       "1564   3.0590  0.0197  0.0086  0.0025   43.5231         -1  \n",
       "1565   3.5662  0.0262  0.0245  0.0075   93.4941         -1  \n",
       "1566   3.6275  0.0117  0.0162  0.0045  137.7844         -1  \n",
       "\n",
       "[1567 rows x 592 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('uci-secom - uci-secom.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d56c65c-1c5f-4aa4-ad69-b163fb427a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 592 entries, Time to Pass/Fail\n",
      "dtypes: float64(590), int64(1), object(1)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62efb6de-4dc2-46f0-885a-b809fddc312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
       "       ...\n",
       "       '581', '582', '583', '584', '585', '586', '587', '588', '589',\n",
       "       'Pass/Fail'],\n",
       "      dtype='object', length=592)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ca112377-2881-43da-b415-7977fc634afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Model Before Feature Selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "834194be-4a93-4be5-805d-51acdaaad1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "Pass/Fail\n"
     ]
    }
   ],
   "source": [
    "# Dropping Time column\n",
    "data = df.drop('Time', axis=1)\n",
    "\n",
    "# Filling NaN values with random\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ca3f4f72-f2fe-4800-abaa-c95c87470365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NaN Values with random \n",
    "for column in data.columns:\n",
    "    # print(\"Column- \", column)\n",
    "    # Get the minimum and maximum values of the column\n",
    "    min_value = data[column].min()\n",
    "    max_value = data[column].max()\n",
    "\n",
    "    # Generate random numbers within the range\n",
    "    random_values = np.random.uniform(min_value, max_value, size=data[column].isnull().sum())\n",
    "    \n",
    "    # Create a Series with the random values\n",
    "    random_series = pd.Series(random_values, index=data[column][data[column].isnull()].index)\n",
    "\n",
    "    # Fill NaN values with the random series\n",
    "    data[column] = data[column].fillna(random_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb965b65-e3f5-446c-8970-104c8d80e972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1253, 590)\n",
      "(314, 590)\n",
      "Test accuracy: 0.9235668789808917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('Pass/Fail', axis=1)\n",
    "y = data['Pass/Fail']\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Initialize and train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1)  # Increase max_iter if it doesn't converge yha pe 1 iterartion me accuracy bahot achi aa rhi chaho to 100,1000 bhi kr skte ho but kyuki 1 hi iter me achi to kya matlab jaba iterartion se \n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5c646-b05c-4240-86ba-413780ab9bac",
   "metadata": {},
   "source": [
    "To perform filter-based feature selection on the \"UCI SECOM\" dataset, which has 592 columns and a target column called \"Pass/Fail,\" we can utilize the following methods:\r\n",
    "\r\n",
    "Duplicate Features:\r\n",
    "\r\n",
    "Identify and remove duplicate columns from the dataset. Columns with identical values provide redundant information and do not contribute to the prediction task.\r\n",
    "Variance Threshold Method:\r\n",
    "\r\n",
    "Calculate the variance of each feature.\r\n",
    "Remove features with low variance, as they tend to have little or no predictive power.\r\n",
    "Set a threshold value for variance and remove features below that threshold.\r\n",
    "Correlation:\r\n",
    "\r\n",
    "Compute the correlation matrix of the features.\r\n",
    "Identify highly correlated features and choose one from each highly correlated group.\r\n",
    "High correlation between features indicates redundancy, and removing one from each correlated group helps reduce multicollinearity.\r\n",
    "ANOVA (Analysis of Variance):\r\n",
    "\r\n",
    "Perform an ANOVA test between each feature and the target variable (\"Pass/Fail\").\r\n",
    "Select features with a significant impact on the target variable.\r\n",
    "Set a significance level (e.g., p-value threshold) for the test to determine the importance of each feature.\r\n",
    "Chi-Squared:\r\n",
    "\r\n",
    "Apply the Chi-Squared test between each feature and the target variable, considering both features as categorical.\r\n",
    "Select features with a significant association with the target variable.\r\n",
    "Set a significance level (e.g., p-value threshold) to determine the importance of each feature.\r\n",
    "Implementing these feature selection methods in Python using the \"UCI SECOM\" dataset can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7566b185-9414-489c-a9c3-86f6e19c58d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns -  592\n",
      "Number of Columns after removing duplicate columns-  487\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "data_path = 'uci-secom - uci-secom.csv'\n",
    "# Load the dataset\n",
    "DATA = pd.read_csv(data_path)  # Replace with the actual filename and path\n",
    "\n",
    "# Remove duplicate features\n",
    "# Get the subset of columns with duplicate values\n",
    "duplicated_cols = DATA.columns[DATA.T.duplicated()]\n",
    "\n",
    "# Remove the duplicated columns\n",
    "data = DATA.drop(columns=duplicated_cols)\n",
    "\n",
    "# Drop time column\n",
    "data.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Numbers Of Columns after removing Duplicate columns\n",
    "print(\"Number of Columns - \", DATA.shape[1])\n",
    "print(\"Number of Columns after removing duplicate columns- \", data.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6990aa8d-a2e0-4e1e-aa3d-aa620da28006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns after Variance Threshold Method-  315\n"
     ]
    }
   ],
   "source": [
    "# Variance Threshold Method\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "sel = selector.fit(data)\n",
    "\n",
    "columns = data.columns[sel.get_support()]\n",
    "\n",
    "data_vt = sel.transform(data)\n",
    "\n",
    "data_vt = pd.DataFrame(data_vt, columns=columns)\n",
    "\n",
    "# Numbers Of Columns after Variance Threshold Method\n",
    "\n",
    "print(\"Number of Columns after Variance Threshold Method- \", data_vt.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "506664ad-5486-444d-ae1f-de1020987db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns after Correlation-  162\n"
     ]
    }
   ],
   "source": [
    "# Correlation\n",
    "corr_matrix = data_vt.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.8)]\n",
    "data_corr = data_vt.drop(to_drop, axis=1)\n",
    "\n",
    "# Numbers Of Columns after Variance Threshold Method\n",
    "\n",
    "print(\"Number of Columns after Correlation- \", data_corr.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a3e61b9a-1421-43fe-b9e3-3360c0cad6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 162 entries, 0 to Pass/Fail\n",
      "dtypes: float64(162)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "data_corr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a45dda27-3c50-4177-96ae-1ae6c63360cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA - Approach showed in the session can't be done here\n",
    "#  every rows of the dataset have some NaN values.\n",
    "# ANOVA can't be applied on data having NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f84290-fca2-41ba-a74d-bc609960c60e",
   "metadata": {},
   "source": [
    "We can still perform ANOVA analysis by considering each column separately.\r\n",
    "\r\n",
    "ANOVA can be applied to each column individually, comparing the target variable which is Pass/Fail column in our case against the non-missing values in that specific column.\r\n",
    "\r\n",
    "We will drop NaN values of each column before performing the ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "509109d0-94e7-479d-a9c6-5311ef9171e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0 - ANOVA p-value: 0.0\n",
      "Column: 1 - ANOVA p-value: 0.0\n",
      "Column: 2 - ANOVA p-value: 0.0\n",
      "Column: 3 - ANOVA p-value: 0.0\n",
      "Column: 4 - ANOVA p-value: 0.0003805229753154665\n",
      "Column: 6 - ANOVA p-value: 0.0\n",
      "Column: 12 - ANOVA p-value: 0.0\n",
      "Column: 14 - ANOVA p-value: 0.0\n",
      "Column: 15 - ANOVA p-value: 0.0\n",
      "Column: 16 - ANOVA p-value: 0.0\n",
      "Column: 18 - ANOVA p-value: 0.0\n",
      "Column: 19 - ANOVA p-value: 0.0\n",
      "Column: 21 - ANOVA p-value: 0.0\n",
      "Column: 22 - ANOVA p-value: 0.0\n",
      "Column: 23 - ANOVA p-value: 0.0\n",
      "Column: 24 - ANOVA p-value: 5.021119420792319e-05\n",
      "Column: 25 - ANOVA p-value: 0.0\n",
      "Column: 28 - ANOVA p-value: 0.0\n",
      "Column: 29 - ANOVA p-value: 0.0\n",
      "Column: 31 - ANOVA p-value: 0.0\n",
      "Column: 32 - ANOVA p-value: 0.0\n",
      "Column: 33 - ANOVA p-value: 0.0\n",
      "Column: 34 - ANOVA p-value: 0.0\n",
      "Column: 35 - ANOVA p-value: 0.0\n",
      "Column: 37 - ANOVA p-value: 0.0\n",
      "Column: 38 - ANOVA p-value: 0.0\n",
      "Column: 39 - ANOVA p-value: 0.0\n",
      "Column: 40 - ANOVA p-value: 0.0\n",
      "Column: 41 - ANOVA p-value: 0.0\n",
      "Column: 43 - ANOVA p-value: 0.0\n",
      "Column: 44 - ANOVA p-value: 0.0\n",
      "Column: 45 - ANOVA p-value: 0.0\n",
      "Column: 47 - ANOVA p-value: 0.0\n",
      "Column: 48 - ANOVA p-value: 0.0\n",
      "Column: 51 - ANOVA p-value: 0.0\n",
      "Column: 55 - ANOVA p-value: 0.0\n",
      "Column: 59 - ANOVA p-value: 1.2552004335164544e-54\n",
      "Column: 61 - ANOVA p-value: 0.0\n",
      "Column: 62 - ANOVA p-value: 0.0\n",
      "Column: 63 - ANOVA p-value: 0.0\n",
      "Column: 64 - ANOVA p-value: 0.0\n",
      "Column: 67 - ANOVA p-value: 0.023676043778288868\n",
      "Column: 68 - ANOVA p-value: 0.0\n",
      "Column: 71 - ANOVA p-value: 0.0\n",
      "Column: 72 - ANOVA p-value: 0.0\n",
      "Column: 74 - ANOVA p-value: 0.0\n",
      "Column: 83 - ANOVA p-value: 0.0\n",
      "Column: 88 - ANOVA p-value: 0.0\n",
      "Column: 90 - ANOVA p-value: 0.0\n",
      "Column: 96 - ANOVA p-value: 0.0\n",
      "Column: 110 - ANOVA p-value: 0.0\n",
      "Column: 111 - ANOVA p-value: 0.0\n",
      "Column: 115 - ANOVA p-value: 0.0\n",
      "Column: 117 - ANOVA p-value: 0.0\n",
      "Column: 120 - ANOVA p-value: 0.0\n",
      "Column: 122 - ANOVA p-value: 0.0\n",
      "Column: 123 - ANOVA p-value: 0.0\n",
      "Column: 125 - ANOVA p-value: 0.0\n",
      "Column: 126 - ANOVA p-value: 0.0\n",
      "Column: 128 - ANOVA p-value: 0.0\n",
      "Column: 129 - ANOVA p-value: 1.0535630316652931e-20\n",
      "Column: 133 - ANOVA p-value: 0.0\n",
      "Column: 134 - ANOVA p-value: 0.0\n",
      "Column: 135 - ANOVA p-value: 0.0\n",
      "Column: 136 - ANOVA p-value: 0.0\n",
      "Column: 137 - ANOVA p-value: 0.0\n",
      "Column: 138 - ANOVA p-value: 0.0\n",
      "Column: 139 - ANOVA p-value: 0.0\n",
      "Column: 142 - ANOVA p-value: 0.0\n",
      "Column: 150 - ANOVA p-value: 0.0\n",
      "Column: 151 - ANOVA p-value: 1.4166449051091726e-76\n",
      "Column: 158 - ANOVA p-value: 0.0\n",
      "Column: 159 - ANOVA p-value: 3.873435589966139e-233\n",
      "Column: 160 - ANOVA p-value: 1.3135685778323096e-263\n",
      "Column: 161 - ANOVA p-value: 5.651686388148162e-260\n",
      "Column: 162 - ANOVA p-value: 8.401617720519068e-164\n",
      "Column: 163 - ANOVA p-value: 0.0\n",
      "Column: 166 - ANOVA p-value: 0.0\n",
      "Column: 167 - ANOVA p-value: 0.0\n",
      "Column: 169 - ANOVA p-value: 0.0\n",
      "Column: 170 - ANOVA p-value: 0.0\n",
      "Column: 175 - ANOVA p-value: 0.0\n",
      "Column: 177 - ANOVA p-value: 0.0\n",
      "Column: 180 - ANOVA p-value: 0.0\n",
      "Column: 181 - ANOVA p-value: 0.0\n",
      "Column: 182 - ANOVA p-value: 0.0\n",
      "Column: 183 - ANOVA p-value: 0.0\n",
      "Column: 184 - ANOVA p-value: 0.0\n",
      "Column: 185 - ANOVA p-value: 0.0\n",
      "Column: 188 - ANOVA p-value: 0.0\n",
      "Column: 195 - ANOVA p-value: 0.0\n",
      "Column: 198 - ANOVA p-value: 0.0\n",
      "Column: 200 - ANOVA p-value: 0.0\n",
      "Column: 201 - ANOVA p-value: 0.0\n",
      "Column: 208 - ANOVA p-value: 0.0\n",
      "Column: 218 - ANOVA p-value: 0.0\n",
      "Column: 223 - ANOVA p-value: 0.0\n",
      "Column: 245 - ANOVA p-value: 5.80826161681738e-107\n",
      "Column: 255 - ANOVA p-value: 0.0\n",
      "Column: 268 - ANOVA p-value: 0.0\n",
      "Column: 269 - ANOVA p-value: 0.0\n",
      "Column: 416 - ANOVA p-value: 0.0\n",
      "Column: 417 - ANOVA p-value: 0.0\n",
      "Column: 418 - ANOVA p-value: 0.0\n",
      "Column: 419 - ANOVA p-value: 9.552745614823107e-257\n",
      "Column: 423 - ANOVA p-value: 0.0\n",
      "Column: 426 - ANOVA p-value: 0.0\n",
      "Column: 429 - ANOVA p-value: 2.8546782689906276e-183\n",
      "Column: 432 - ANOVA p-value: 7.812152995013836e-189\n",
      "Column: 433 - ANOVA p-value: 7.717688534101183e-240\n",
      "Column: 438 - ANOVA p-value: 0.0\n",
      "Column: 439 - ANOVA p-value: 0.0\n",
      "Column: 442 - ANOVA p-value: 0.0\n",
      "Column: 443 - ANOVA p-value: 0.0\n",
      "Column: 444 - ANOVA p-value: 0.0\n",
      "Column: 460 - ANOVA p-value: 0.0\n",
      "Column: 468 - ANOVA p-value: 6.409943849569832e-267\n",
      "Column: 472 - ANOVA p-value: 0.0\n",
      "Column: 474 - ANOVA p-value: 0.0\n",
      "Column: 476 - ANOVA p-value: 0.0\n",
      "Column: 482 - ANOVA p-value: 0.0\n",
      "Column: 483 - ANOVA p-value: 1.92856351133e-312\n",
      "Column: 484 - ANOVA p-value: 2.61571078101574e-284\n",
      "Column: 485 - ANOVA p-value: 9.761371762975727e-244\n",
      "Column: 486 - ANOVA p-value: 2.6860343455175406e-303\n",
      "Column: 487 - ANOVA p-value: 3.675565124765342e-238\n",
      "Column: 488 - ANOVA p-value: 0.0\n",
      "Column: 489 - ANOVA p-value: 0.0\n",
      "Column: 491 - ANOVA p-value: 0.0\n",
      "Column: 492 - ANOVA p-value: 0.0\n",
      "Column: 493 - ANOVA p-value: 0.0\n",
      "Column: 494 - ANOVA p-value: 4.251939667888049e-27\n",
      "Column: 496 - ANOVA p-value: 0.0\n",
      "Column: 499 - ANOVA p-value: 1.3626860172168723e-196\n",
      "Column: 500 - ANOVA p-value: 2.2063853113344103e-170\n",
      "Column: 510 - ANOVA p-value: 0.0\n",
      "Column: 511 - ANOVA p-value: 8.521260175759374e-208\n",
      "Column: 519 - ANOVA p-value: 7.913113072865541e-183\n",
      "Column: 520 - ANOVA p-value: 1.0658612203668937e-122\n",
      "Column: 521 - ANOVA p-value: 1.7490564973490379e-06\n",
      "Column: 523 - ANOVA p-value: 3.9933876625832445e-35\n",
      "Column: 525 - ANOVA p-value: 0.0\n",
      "Column: 526 - ANOVA p-value: 0.0\n",
      "Column: 539 - ANOVA p-value: 0.0\n",
      "Column: 546 - ANOVA p-value: 0.0\n",
      "Column: 547 - ANOVA p-value: 0.0\n",
      "Column: 548 - ANOVA p-value: 0.0\n",
      "Column: 549 - ANOVA p-value: 0.0\n",
      "Column: 550 - ANOVA p-value: 0.0\n",
      "Column: 551 - ANOVA p-value: 0.0\n",
      "Column: 559 - ANOVA p-value: 0.0\n",
      "Column: 562 - ANOVA p-value: 0.0\n",
      "Column: 563 - ANOVA p-value: 0.0\n",
      "Column: 564 - ANOVA p-value: 0.0\n",
      "Column: 569 - ANOVA p-value: 0.0\n",
      "Column: 570 - ANOVA p-value: 0.0\n",
      "Column: 571 - ANOVA p-value: 0.0\n",
      "Column: 572 - ANOVA p-value: 4.082424901870454e-40\n",
      "Column: 573 - ANOVA p-value: 0.0\n",
      "Column: 581 - ANOVA p-value: 2.0775704433733e-310\n",
      "Column: 585 - ANOVA p-value: 2.7656e-319\n",
      "Number of Columns after Correlation-  101\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Significance Value\n",
    "alpha = 0.05 \n",
    "\n",
    "# Columns haveing p_value less than alpha\n",
    "column_pvalues = []\n",
    "\n",
    "# Iterate over each column\n",
    "for column in data_corr.iloc[:, :-1].columns:\n",
    "    # Extract the non-missing values in the column\n",
    "    column_data = data_corr[column].dropna()\n",
    "    \n",
    "    # Perform ANOVA with the target variable\n",
    "    anova_result = f_oneway(column_data, data_corr['Pass/Fail'])\n",
    "    \n",
    "    # Print the ANOVA result or perform further analysis\n",
    "    print(f\"Column: {column} - ANOVA p-value: {anova_result.pvalue}\")\n",
    "\n",
    "    if anova_result.pvalue <= alpha:\n",
    "        column_pvalues.append((column, anova_result.pvalue))\n",
    "\n",
    "# Selecting best 100 Features - lower p-value better feature\n",
    "# Sort the column p-values in ascending order\n",
    "column_pvalues.sort(key=lambda x: x[1])\n",
    "\n",
    "# Select the top 100 columns with the lowest p-values\n",
    "selected_columns = [column for column, _ in column_pvalues[:100]]\n",
    "\n",
    "data_anova = data_corr[selected_columns+['Pass/Fail']]\n",
    "\n",
    "print(\"Number of Columns after Correlation- \", data_anova.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "441e1746-7fef-4882-a9a0-287ca22cd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi-Squared\n",
    "# -> Our Data is Numerical so chi-squared can't be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "607ae8f9-17d8-438c-84e8-38f0d4f162b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape -  (1567, 101)\n",
      "(1253, 100)\n",
      "(314, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23112\\1039137554.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_anova[column].fillna(random_series, inplace=True)\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_23112\\1039137554.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_anova[column].fillna(random_series, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.910828025477707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Shape of Data after Feature Selection\n",
    "\n",
    "print(\"Shape - \", data_anova.shape)\n",
    "\n",
    "# Filling NaN Values with random \n",
    "for column in data_anova.columns:\n",
    "    # print(\"Column- \", column)\n",
    "    # Get the minimum and maximum values of the column\n",
    "    min_value = data_anova[column].min()\n",
    "    max_value = data_anova[column].max()\n",
    "\n",
    "    # Generate random numbers within the range\n",
    "    random_values = np.random.uniform(min_value, max_value, size=data_anova[column].isnull().sum())\n",
    "    \n",
    "    # Create a Series with the random values\n",
    "    random_series = pd.Series(random_values, index=data_anova[column][data_anova[column].isnull()].index)\n",
    "\n",
    "    # Fill NaN values with the random series\n",
    "    data_anova[column].fillna(random_series, inplace=True)\n",
    "\n",
    "    # Print\n",
    "    # print(data[column].isnull().sum())\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = data_anova.drop('Pass/Fail', axis=1)\n",
    "y = data_anova['Pass/Fail']\n",
    "\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "\n",
    "# Initialize and train logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=10000)  # Increase max_iter if it doesn't converge\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a6e645-a7c4-430b-b2be-53b410d54d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
